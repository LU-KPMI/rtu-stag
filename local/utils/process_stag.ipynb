{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_depth = 21\n",
    "run_trimmed = True\n",
    "\n",
    "kraken_data = {}\n",
    "\n",
    "# work through all of the kraken stuff\n",
    "for fname in glob.glob(\"outputs/*/kraken2/*.kreport\"):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read().split(\"\\n\") # split data into separate rows\n",
    "        parsed_data = {}\n",
    "        \n",
    "        curr_tag = ['-' for i in range(0, max_depth)]\n",
    "        for i in data:\n",
    "            curr_row = i.split(\"\\t\") # the data within rows is tab-delimited\n",
    "\n",
    "            # create a tag structure\n",
    "            curr_n = curr_row[-1].split(\"  \")\n",
    "    \n",
    "            if curr_n[-1] != '': # skip rows w-out taxon names (mostly to deal w empty entries)\n",
    "                try:\n",
    "                    curr_tag[len(curr_n)-1] = curr_n[-1]\n",
    "                except:\n",
    "                    print(curr_n) # just a contingency to see when we end up deeper than expected\n",
    "                    \n",
    "                for j in range(len(curr_n), max_depth):\n",
    "                    curr_tag[j] = '-'\n",
    "                    \n",
    "                # extract the percentage of aligned reads\n",
    "                #parsed_data['\\t'.join(curr_tag)] = curr_row[0].replace(\" \", \"\")\n",
    "                parsed_data['\\t'.join(curr_tag)] = curr_row[1].replace(\" \", \"\")\n",
    "                #parsed_data['\\t'.join(curr_tag)] = curr_row[2].replace(\" \", \"\")\n",
    "                \n",
    "        kraken_data[fname.split(\"/\")[-3].split(\"_\")[2]] = dict(parsed_data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim down to a specific level for visualization purposes\n",
    "if run_trimmed:\n",
    "    target_level = 5\n",
    "\n",
    "    kraken_tmp = {}\n",
    "\n",
    "    for s, sv in kraken_data.items():\n",
    "        kraken_tmp[s] = {}\n",
    "\n",
    "        for k, v in sv.items():\n",
    "            tmp_k = k.split(\"\\t\")\n",
    "\n",
    "            if tmp_k[target_level] != \"-\" and tmp_k[target_level+1] == \"-\":\n",
    "                kraken_tmp[s][tmp_k[target_level]] = v\n",
    "\n",
    "    kraken_data = kraken_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to organize in a way that keeps a logical order intact\n",
    "keylist = []\n",
    "\n",
    "for k, e in kraken_data.items():\n",
    "    for s, l in e.items():\n",
    "        keylist.append(s) # just joining it up to make filtering duplicates out easier\n",
    "        \n",
    "keylist = [x.split('\\t') for x in list(set(keylist))]\n",
    "     \n",
    "if not run_trimmed:\n",
    "    for i in range(1, max_depth+1):\n",
    "        keylist = sorted(keylist, key=lambda x: x[-i]) # need to sort it starting from the right col to keep it from re-sorting everything\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kraken2.csv', 'w') as f:\n",
    "    kraken_keys = list(kraken_data.keys())\n",
    "    f.write(\",\".join(['' for i in range(0, max_depth)]+kraken_keys)+\"\\n\")\n",
    "\n",
    "    for kl in keylist: # work through all of sorted rows \n",
    "        tmp_row = list(kl)\n",
    "        tmp_key = '\\t'.join(kl)\n",
    "        \n",
    "        for s in kraken_keys:\n",
    "            if tmp_key in kraken_data[s]:\n",
    "                tmp_row.append(kraken_data[s][tmp_key])\n",
    "            else:\n",
    "                tmp_row.append('NaN')\n",
    "    \n",
    "        f.write(\",\".join(tmp_row)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "groot_data = {}\n",
    "\n",
    "# not sure what to do with the groot stuff\n",
    "for fname in glob.glob(\"outputs/*/groot/*/*.txt\"):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read().split(\"\\n\") # split data into separate rows\n",
    "        \n",
    "        tmp_data = {}\n",
    "        \n",
    "        for e in data:\n",
    "            tmp_val = e.split(\"\\t\")\n",
    "            \n",
    "            if tmp_val[0] != '':\n",
    "                tmp_data[tmp_val[0]] = tmp_val[1:]\n",
    "            \n",
    "        groot_data[fname.split(\"/\")[-4].split(\"_\")[2]] = tmp_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to organize in a way that keeps a logical order intact\n",
    "keylist = []\n",
    "\n",
    "for k, e in groot_data.items():\n",
    "    for s, l in e.items():\n",
    "        keylist.append(s) # just joining it up to make filtering duplicates out easier\n",
    "        \n",
    "keylist = list(set(keylist))\n",
    "keylist.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "affixes = [\"_read_count\", \"_gene_length\", \"_coverage\"]\n",
    "\n",
    "with open('groot.csv', 'w') as f:\n",
    "    groot_keys = list(groot_data.keys())\n",
    "    tmp_row = ['']\n",
    "    \n",
    "    for x in groot_keys:\n",
    "        for a in affixes:\n",
    "            tmp_row.append(x+a)\n",
    "    f.write(\",\".join(tmp_row)+\"\\n\")\n",
    "\n",
    "    for kl in keylist: # work through all of sorted rows \n",
    "        tmp_row = [kl]\n",
    "        \n",
    "        for s in groot_keys:\n",
    "            for i in range(0, len(affixes)):\n",
    "                if kl in groot_data[s]:\n",
    "                    tmp_row.append(groot_data[s][kl][i])\n",
    "                else:\n",
    "                    tmp_row.append('NaN')\n",
    "    \n",
    "        f.write(\",\".join(tmp_row)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphlan_data = {}\n",
    "\n",
    "# not sure what to do with the groot stuff\n",
    "for fname in glob.glob(\"outputs/*/metaphlan/*.txt\"):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read().split(\"\\n\") # split data into separate rows\n",
    "        \n",
    "        tmp_data = {}\n",
    "        \n",
    "        for e in data[4:]:\n",
    "            tmp_val = e.split(\"\\t\")\n",
    "            \n",
    "            if tmp_val[0] != '':\n",
    "                tmp_data[tmp_val[0]] = tmp_val[2]\n",
    "            \n",
    "        metaphlan_data[fname.split(\"/\")[-3].split(\"_\")[2]] = tmp_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to organize in a way that keeps a logical order intact\n",
    "keylist = []\n",
    "\n",
    "for k, e in metaphlan_data.items():\n",
    "    for s, l in e.items():\n",
    "        keylist.append(s) # just joining it up to make filtering duplicates out easier\n",
    "        \n",
    "keylist = [x.split('|') for x in list(set(keylist))]\n",
    "        \n",
    "max_padding = 0\n",
    "for x in keylist:\n",
    "    if len(x) > max_padding:\n",
    "        max_padding = len(x)\n",
    "    \n",
    "for x in keylist:\n",
    "    for i in range(0, max_padding-len(x)):\n",
    "        x.append(\"-\")\n",
    "    \n",
    "for i in range(1, max_padding+1):\n",
    "    keylist = sorted(keylist, key=lambda x: x[-i]) # need to sort it starting from the right col to keep it from re-sorting everything\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metaphlan.csv', 'w') as f:\n",
    "    metaphlan_keys = list(metaphlan_data.keys())\n",
    "    f.write(\",\".join(['' for i in range(0, max_padding)]+metaphlan_keys)+\"\\n\")\n",
    "\n",
    "    for kl in keylist: # work through all of sorted rows \n",
    "        tmp_row = list(kl)\n",
    "        tmp_key = '|'.join([p for p in kl if p != \"-\"])\n",
    "        \n",
    "        for s in metaphlan_keys:\n",
    "            if tmp_key in metaphlan_data[s]:\n",
    "                tmp_row.append(metaphlan_data[s][tmp_key])\n",
    "            else:\n",
    "                tmp_row.append('NaN')\n",
    "    \n",
    "        f.write(\",\".join(tmp_row)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outputs/output_dir_101_202006040145/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_101_202006040145/humann2/101_pathabundance_relab.tsv', 'outputs/output_dir_101_202006040145/humann2/101_pathabundance.tsv', 'outputs/output_dir_101_202006040145/humann2/101_pathcoverage.tsv', 'outputs/output_dir_101_202006040145/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_101_202006040145/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_101_202006040145/humann2/101_genefamilies.tsv', 'outputs/output_dir_101_202006040145/humann2/101_genefamilies_relab.tsv', 'outputs/output_dir_14_202006042053/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_14_202006042053/humann2/14_pathabundance.tsv', 'outputs/output_dir_14_202006042053/humann2/14_pathcoverage.tsv', 'outputs/output_dir_14_202006042053/humann2/14_genefamilies_relab.tsv', 'outputs/output_dir_14_202006042053/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_14_202006042053/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_14_202006042053/humann2/14_genefamilies.tsv', 'outputs/output_dir_14_202006042053/humann2/14_pathabundance_relab.tsv', 'outputs/output_dir_2_202006050553/humann2/2_pathabundance.tsv', 'outputs/output_dir_2_202006050553/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_2_202006050553/humann2/2_pathcoverage.tsv', 'outputs/output_dir_2_202006050553/humann2/2_genefamilies_relab.tsv', 'outputs/output_dir_2_202006050553/humann2/2_pathabundance_relab.tsv', 'outputs/output_dir_2_202006050553/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_2_202006050553/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_2_202006050553/humann2/2_genefamilies.tsv', 'outputs/output_dir_3_202006050809/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_3_202006050809/humann2/3_genefamilies.tsv', 'outputs/output_dir_3_202006050809/humann2/3_pathabundance_relab.tsv', 'outputs/output_dir_3_202006050809/humann2/3_genefamilies_relab.tsv', 'outputs/output_dir_3_202006050809/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_3_202006050809/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_3_202006050809/humann2/3_pathcoverage.tsv', 'outputs/output_dir_3_202006050809/humann2/3_pathabundance.tsv', 'outputs/output_dir_16_202006050212/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_16_202006050212/humann2/16_pathcoverage.tsv', 'outputs/output_dir_16_202006050212/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_16_202006050212/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_16_202006050212/humann2/16_pathabundance.tsv', 'outputs/output_dir_16_202006050212/humann2/16_pathabundance_relab.tsv', 'outputs/output_dir_16_202006050212/humann2/16_genefamilies.tsv', 'outputs/output_dir_16_202006050212/humann2/16_genefamilies_relab.tsv', 'outputs/output_dir_4_202006051141/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_4_202006051141/humann2/4_pathcoverage.tsv', 'outputs/output_dir_4_202006051141/humann2/4_pathabundance_relab.tsv', 'outputs/output_dir_4_202006051141/humann2/4_pathabundance.tsv', 'outputs/output_dir_4_202006051141/humann2/4_genefamilies_relab.tsv', 'outputs/output_dir_4_202006051141/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_4_202006051141/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_4_202006051141/humann2/4_genefamilies.tsv', 'outputs/output_dir_15_202006042150/humann2/15_pathabundance.tsv', 'outputs/output_dir_15_202006042150/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_15_202006042150/humann2/15_genefamilies.tsv', 'outputs/output_dir_15_202006042150/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_15_202006042150/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_15_202006042150/humann2/15_pathabundance_relab.tsv', 'outputs/output_dir_15_202006042150/humann2/15_pathcoverage.tsv', 'outputs/output_dir_15_202006042150/humann2/15_genefamilies_relab.tsv', 'outputs/output_dir_103_202006040738/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_103_202006040738/humann2/103_genefamilies_relab.tsv', 'outputs/output_dir_103_202006040738/humann2/103_genefamilies.tsv', 'outputs/output_dir_103_202006040738/humann2/103_pathabundance.tsv', 'outputs/output_dir_103_202006040738/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_103_202006040738/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_103_202006040738/humann2/103_pathabundance_relab.tsv', 'outputs/output_dir_103_202006040738/humann2/103_pathcoverage.tsv', 'outputs/output_dir_1_202006041359/humann2/1_genefamilies_relab.tsv', 'outputs/output_dir_1_202006041359/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_1_202006041359/humann2/1_pathabundance.tsv', 'outputs/output_dir_1_202006041359/humann2/1_pathabundance_relab.tsv', 'outputs/output_dir_1_202006041359/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_1_202006041359/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_1_202006041359/humann2/1_pathcoverage.tsv', 'outputs/output_dir_1_202006041359/humann2/1_genefamilies.tsv', 'outputs/output_dir_97_202006051436/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_97_202006051436/humann2/97_genefamilies.tsv', 'outputs/output_dir_97_202006051436/humann2/97_pathabundance_relab.tsv', 'outputs/output_dir_97_202006051436/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_97_202006051436/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_97_202006051436/humann2/97_pathcoverage.tsv', 'outputs/output_dir_97_202006051436/humann2/97_genefamilies_relab.tsv', 'outputs/output_dir_97_202006051436/humann2/97_pathabundance.tsv', 'outputs/output_dir_98_202006051822/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_98_202006051822/humann2/98_genefamilies.tsv', 'outputs/output_dir_98_202006051822/humann2/98_genefamilies_relab.tsv', 'outputs/output_dir_98_202006051822/humann2/98_pathabundance.tsv', 'outputs/output_dir_98_202006051822/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_98_202006051822/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_98_202006051822/humann2/98_pathcoverage.tsv', 'outputs/output_dir_98_202006051822/humann2/98_pathabundance_relab.tsv', 'outputs/output_dir_99_202006052139/humann2/99_pathcoverage.tsv', 'outputs/output_dir_99_202006052139/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_99_202006052139/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_99_202006052139/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_99_202006052139/humann2/99_pathabundance_relab.tsv', 'outputs/output_dir_99_202006052139/humann2/99_genefamilies_relab.tsv', 'outputs/output_dir_99_202006052139/humann2/99_pathabundance.tsv', 'outputs/output_dir_99_202006052139/humann2/99_genefamilies.tsv', 'outputs/output_dir_100_202006040319/humann2/100_pathcoverage.tsv', 'outputs/output_dir_100_202006040319/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_100_202006040319/humann2/100_pathabundance_relab.tsv', 'outputs/output_dir_100_202006040319/humann2/100_genefamilies.tsv', 'outputs/output_dir_100_202006040319/humann2/100_pathabundance.tsv', 'outputs/output_dir_100_202006040319/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_100_202006040319/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_100_202006040319/humann2/100_genefamilies_relab.tsv', 'outputs/output_dir_13_202006041516/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_13_202006041516/humann2/13_pathabundance.tsv', 'outputs/output_dir_13_202006041516/humann2/13_pathcoverage.tsv', 'outputs/output_dir_13_202006041516/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_13_202006041516/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_13_202006041516/humann2/13_pathabundance_relab.tsv', 'outputs/output_dir_13_202006041516/humann2/13_genefamilies_relab.tsv', 'outputs/output_dir_13_202006041516/humann2/13_genefamilies.tsv', 'outputs/output_dir_104_202006040818/humann2/104_pathcoverage.tsv', 'outputs/output_dir_104_202006040818/humann2/all_samples.humann2_pathabundance.tsv', 'outputs/output_dir_104_202006040818/humann2/104_genefamilies_relab.tsv', 'outputs/output_dir_104_202006040818/humann2/104_genefamilies.tsv', 'outputs/output_dir_104_202006040818/humann2/all_samples.humann2_genefamilies.tsv', 'outputs/output_dir_104_202006040818/humann2/all_samples.humann2_pathcoverage.tsv', 'outputs/output_dir_104_202006040818/humann2/104_pathabundance_relab.tsv', 'outputs/output_dir_104_202006040818/humann2/104_pathabundance.tsv']\n"
     ]
    }
   ],
   "source": [
    "# work through all of the humann2 stuff\n",
    "print(glob.glob(\"outputs/*/humann2/*.tsv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
